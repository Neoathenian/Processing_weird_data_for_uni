# Preprocesamiento

## Introducción

A lo largo de este Markdown procesaremos los datos. Los datos que veremos son en general datos con colas muy pesadas, por lo que para eliminar las distintas escalas a las que se encuentran los datos aplicaremos log(1+x) o alguna variación de ella para los que presentan este tipo de distribución. Nuestro objetivo con esto es obtener buenos resultados al normalizar los datos, pues los resultados son mucho peores con variables multiescala.

Este proceso lo haremos inicialmente variable a variable pues queremos hacer un análisis de sus valores faltantes para adaptar el tipo de imputación adecuadamente. En general veremos los NAs se deben a divisiones por 0, en cuyo caso, si estamos ante una distribución positiva imputaremos por el máximo. En caso de saber no saber el motivo por el que se ha producido un NA, o la distribución no es positiva, aplicaremos imputación mediante kNN.

Lo que se presentará en este documento es el código y el razonamiento que se llevó a cabo, desarrollándose mas las ideas y motivos de las primeras transformaciones y posteriormente menos al ser ideas ejecutadas al principio.

Dejamos un resumen de las transformaciones aplicadas a cada variable en el anexo: **Transformaciones_preprocesamiento.txt**

Lo primero que haremos será importar los datos

```{r}
root.dir="E:/DeMesa/Master_BigData/Mineria_de_datos/Trabajo/Datasets_Raw/"
df_raw=read.csv(paste0(root.dir,"Bancarrota3years.csv"), row.names=1, na.strings="?")
df_raw$class=as.factor(df_raw$class)
```

Vemos que hay muchísimos NAs comparado con el número de filas

```{r}
cat(paste0("Nº NAs: ",sum(is.na(df_raw)),"\nNº filas: ",dim(df_raw)[1]))
```

## Tratamiento NAs y de outliers

A continuación trataremos los NAs. Para ello, buscaremos que variables tienen NAs y buscaremos una posible causa.

Mirando en la siguiente gráfica, podemos ver que la variable con mayor cantidad de valores faltantes es el atributo 37, que de hecho tiene 4736 valores faltantes. Sabiendo que el dataset tiene 10503 datos, esto significa que aproximadamente la mitad de sus valores son faltantes.

```{r}
#library(Hmisc)
#missing_vals_description=describe(df_raw)
#plot(missing_vals_description)
library(VIM)
aggr(df_raw)

```

El attributo 37 es *(activo circulante - existencias) / pasivo a largo plazo*. Por lo tanto, el motivo mas probable por el que estos valores son NAs es que estamos dividiendo por 0; es decir, pasivo a largo plazo es 0. Este patrón lo veremos numerosas veces a lo largo del dataset pues todos los atributos son ratios. Para poder confirmar o refutar este tipo de hipótesis, buscaremos otras variables que tengan el denominador problemático en algún lugar del ratio que nos permita determinar si vale o no 0. Así, si dos variables siempre son NA a la vez, y coincide que tienen el mismo denominador, entonces asumiremos que es debido a que el denominador es originalmente 0.

Debido a que asumiremos que estamos dividiendo por 0, lo lógico es sustituir por un valor muy alto; posiblemente el máximo de los valores que hayamos encontrado. Sin embargo, como posteriormente queremos realizar modelos sensibles a outliers realizaremos un estudio de los outliers de la variable a imputar a la vez, viendo si hay alguna solución para ellos para así no asignar los valores faltantes a valores desorbitadamente altos. Es decir, implementaremos el tratamiento de outliers y de NAs simultáneamente.

### Atributo 37

El atributo 59 es:

-   *Attr59 pasivo a largo plazo / fondos propios*

Por tanto, si los NAs del atributo 37 se deben a divisiones por 0, en dichos casos veremos que el atributo 59 es 0. Esto es justo lo que podemos ver a continuación:

```{r}
table(is.na(df_raw[!is.na(df_raw$Attr59),]$Attr37),df_raw[!is.na(df_raw$Attr59),]$Attr59==0)
```

En esta tabla, podemos ver que los NA del attributo 37 son equivalentes a los 0s del atributo 59 salvo en 3 valores. Podemos ver, que en estos 3 valores, el atributo vale 0, lo que parece indicar una de dos cosas: o sabían que estaban dividiendo por 0 y pusieron un 0, o el numerador en ese caso también fue 0, por lo que ni se molestaron en dividir. Estos 3 valores los ignoraremos, y solo imputaremos los que ya son NA.

Ahora miraremos como imputar los NAs del Attr37, pero antes, miraremos su distribución para tratar los outliers

El atributo 37 resulta muy interesante, debido a que aparece en diversas escalas. Es decir, si nos movemos en múltiplos de 10 sigue habiendo una gran cantidad de valores. De hecho, estos múltiplos de 10 coinciden con el 3º cuantil. Mostramos a continuación los cuantiles de esta variable para los casos: normal, Attr37\>10 y Attr37\>100.

```{r}
print(paste0("Raw, nºrows: ",length(df_raw[df_raw$Attr37 & !is.na(df_raw$Attr37),"Attr37"])))
print(summary(df_raw[df_raw$Attr37 & !is.na(df_raw$Attr37),"Attr37"]))
print(paste0(">10, nºrows: ",length(df_raw[df_raw$Attr37>10 & !is.na(df_raw$Attr37),"Attr37"])))
print(summary(df_raw[df_raw$Attr37>10 & !is.na(df_raw$Attr37),"Attr37"]))
print(paste0(">100, nºrows: ",length(df_raw[df_raw$Attr37>100 & !is.na(df_raw$Attr37),"Attr37"])))
print(summary(df_raw[df_raw$Attr37>100 & !is.na(df_raw$Attr37),"Attr37"]))
```

Una herramienta para lidiar con las variables multiescala es el logaritmo, pero para ello es necesario que la variable sea mayor estrictamente que 0. Este no es el caso para nuestra variable; sin embargo, como veremos a continuación esto solo es problema para 6 filas, de las cuales se puede comprobar que 3 son menores que 0 y 3 son exactamente 0.

```{r}
sum(df_raw$Attr37<=0,na.rm=T)
```

Visto esto, lo que haremos con esta variable será imputar los valores menores o igual que 0 por el mínimo (0.00685). Luego le aplicaremos el logaritmo neperiano. Después limitaremos los valores usando la frontera de outlier; es decir, 1ºcuantil-1.5*IQR y 3ºcuantil+1.5*IQR y finalmente, imputaremos los valores faltantes por ese máximo.

```{r}
num_outliers <- function(x) {
  q1 <- quantile(x, 0.25,na.rm=T)
  q3 <- quantile(x, 0.75,na.rm=T)
  iqr <- q3 - q1
  upper <- q3 + 1.5 * iqr
  lower <- q1 - 1.5 * iqr
  outliers_lower <- sum(x < lower)
  outliers_upper <- sum(x > upper)
  return(c(outliers_lower,outliers_upper))
}

impute_outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  upper <- q3 + 1.5 * iqr
  lower <- q1 - 1.5 * iqr
  x[x < lower] <- lower
  x[x > upper] <- upper
  return(x)
}


```

```{r}
df_raw[df_raw$Attr37<=0 & !is.na(df_raw$Attr37),"Attr37"]=min(df_raw$Attr37[df_raw$Attr37>0],na.rm=T)
df_raw[!is.na(df_raw$Attr37),"Attr37"]=log(df_raw[!is.na(df_raw$Attr37),"Attr37"])
num_outliers(df_raw[!is.na(df_raw$Attr37),"Attr37"])
summary(df_raw[!is.na(df_raw$Attr37),"Attr37"])

```

Hasta ahora hemos aplicado el logaritmo neperiano, y podemos ver que tenemos un rango intercuartílico de 2.3, 25 outliers por debajo del 1º cuartil y 140 por encima. Como mencionamos, estos los transformaremos en su límite correspondiente.

```{r}
df_raw[!is.na(df_raw$Attr37),"Attr37"]=impute_outliers(df_raw[!is.na(df_raw$Attr37),"Attr37"])
df_raw[is.na(df_raw$Attr37),"Attr37"]=max(df_raw[!is.na(df_raw$Attr37),"Attr37"])
```

### Atributo 21

Los siguientes atributos con muchos NAs son:

-   *Attr21 ventas (n) / ventas (n-1)*

-   *Attr27 resultado de explotación / gastos financieros*

-   *Attr45 beneficio neto / existencias*

-   *Attr60 ventas / existencias*

Por este motivo, iremos en orden mirando uno a uno. Podemos ver que todos tienen mas de 500 NAs:

```{r}
colSums(is.na(df_raw))[colSums(is.na(df_raw))>500]
```

En el 21 parece claro que los NAs se deberán a que no hubo ventas anteriormente. Esto no podemos demostrarlo por desgracia, pero como trabajaremos bajo esta hipótesis, invertiremos esta variable, convirtiendo a los NAs en 0s; y los 0s que ya tenía esta variable (hay 9), los mantendremos como 0s, correspondiendo a un caso de 0/0.

```{r}
df_raw[!is.na(df_raw$Attr21) & (df_raw$Attr21!=0),"Attr21"]=1/df_raw[!is.na(df_raw$Attr21) & (df_raw$Attr21!=0),"Attr21"]
df_raw[is.na(df_raw$Attr21),"Attr21"]=0
```

Mirando un summary de esta variable, vemos inmediatamente dos cosas sorprendentes: 1. Hay valores negativos, que se puede entender desde el punto de vista de devoluciones, pero no tiene mucho sentido. 2. Outliers enormes. Estos probablemente se deban a que la empresa ha empezado a prosperar pues ventas(n)\>\>ventas(n-1). Vemos que realmente no hay muchos de ningunos de estos, por lo que simplemente los imputaremos.

```{r}
summary(df_raw$Attr21)
print(paste0("NºValores negativos: ",sum(df_raw$Attr21<0)))
print(paste0("NºValores mayor a 4: ",sum(df_raw$Attr21>4)))
```

Para los negativos, los imputaremos por 0, y luego aplicaremos log(1+x) para juntar las distintas escalas a las que se encuentran los datos, manteniendo el 0 en el 0.

```{r}
df_raw[df_raw$Attr21<0,"Attr21"]=0
df_raw[df_raw$Attr21>0,"Attr21"]=log(1+df_raw[df_raw$Attr21>0,"Attr21"])
```

### Atributo 27

Este atributo no tiene una buena comparativa con ningún otro atributo. El mas cercano es el *Atributo 11 (beneficio bruto + extraordinarios + gastos financieros) / activo total*. Debido a que la variable 27 puede ser negativa, poder deducir que los NAs del atributo 27 corresponden con que una de las partes del numerador sea 0, lo hace incluso menos probable. Sin embargo, en el siguiente histograma, en el que ploteamos el histograma del atributo 11 dividido en varias zonas: 1. Cuando el atributo 27 es 0 2. Cuando el atributo 27 es mayor que 1 3. Cuando el atributo 27 es menor que -1 La idea, es que como el histograma se mueve a la derecha para valores mayor que 1 y a la izquierda para valores menor que -1 (respecto a los que son NA en el atributo 27); entonces, tenemos que si es posible que los NAs correspondan a 0s (aunque no son pruebas concluyentes).

```{r}
#plot(1, type="n", xlim=c(0, max(aux$Attr11)), ylim=c(0, 100))
aux=df_raw[df_raw$Attr11>-0.5 & df_raw$Attr11< 0.5,]

hist(aux$Attr11[is.na(aux$Attr27)],breaks=100,col=rgb(0,1,0,0.7),freq =F,main=NULL,xlab="Attr11")
hist(aux$Attr11[!is.na(aux$Attr27) & (aux$Attr27> 1)],breaks=100,col=rgb(1,0,0,0.4),freq =F,add=T)
hist(aux$Attr11[!is.na(aux$Attr27) & (aux$Attr27< -1)],breaks=100,col=rgb(0,0,1,0.4),freq =F,add=T)
title(main="Histograma Attr11 variando Attr27")

legend("topright", legend=c("Attr27 is NA", "Attr27>1","Attr27< -1"),
       fill=c(rgb(0, 1,0, 0.7), rgb(1, 0, 0, 0.4),rgb(0,0,1,0.4)))
```

Aunque el histograma verde no es completamente un pico entre medias de la zona roja y la zona azul, y de hecho se mete bastante en la zona roja, asumiremos que la hipótesis de que los NAs son 0s es correcta, y procederemos a la imputación mediante knn tras mirar detenidamente los outliers, de los cuales vemos que hay muchos (unos 2000). Usamos el método de kNN, porque aunque podemos asumir que el denominador sea 0, el numerador puede ser positivo o negativo.

```{r}
num_outliers(df_raw[!is.na(df_raw$Attr27),"Attr27"])
summary(df_raw$Attr27)
print(paste0("valores muy alejados:", sum(!(df_raw$Attr27<20 & df_raw$Attr27>-20 & !is.na(df_raw$Attr27)))))
```

Viendo que hay valores tan grandes (los outliers son muy extremos), y que hay muchos (20%), parece que estamos ante otro caso de variable multiescala. Esto podemos confirmarlo fácilmente viendo cuantos valores hay en cada escala, para lo cual miraremos por simplicidad cuantos hay por encima de cada potencia de 10, que vemos que son una cantidad no trivial:

```{r}
print(paste0("Nº>10: ",sum(df_raw$Attr27>10 & !is.na(df_raw$Attr27))))
print(paste0("Nº>100: ",sum(df_raw$Attr27>100 & !is.na(df_raw$Attr27))))
print(paste0("Nº>1000: ",sum(df_raw$Attr27>1000 & !is.na(df_raw$Attr27))))
print(paste0("Nº>1000: ",sum(df_raw$Attr27>1000 & !is.na(df_raw$Attr27))))
print(paste0("Nº>10000: ",sum(df_raw$Attr27>10000 & !is.na(df_raw$Attr27))))
```

Para tratar las distintas escalas usaremos sign(x)\*log(1+\|x\|) pues mantiene la monotonía de la variable, su signo y junta las distintas escalas.

```{r}
df_raw[!is.na(df_raw$Attr27),"Attr27"]=sign(df_raw[!is.na(df_raw$Attr27),"Attr27"])*log(1+abs(df_raw[!is.na(df_raw$Attr27),"Attr27"]))
summary(df_raw$Attr27)
```

Aunque siguen quedando outliers, los valores que han quedado son mucho mas manejables en cuanto a modelado, por lo que a continuación procederemos a hacer knn.

```{r}
library(VIM)
# Imputar valores faltantes utilizando el método KNN de VIM
imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = "Attr27")
# Reemplazar la columna original con la columna imputada
df_raw$Attr27 <- imputed_data$Attr27
```

### Atributos 45 y 60

Los atributos 45 y 60 tienen aproximadamente el mismo número de NAs y tienen el mismo denominador. Es claro que esto se debe a un caso de división por 0. Como podemos ver en la siguiente tabla, todos los NAs (591) del atributo 45 son NAs de del atributo 60. Estos atributos son:

-   *Attr45 beneficio neto / existencias*

-   *Attr60 ventas / existencias*

```{r}
table(is.na(df_raw$Attr45),is.na(df_raw$Attr60))
```

Vemos que queda un NA en el atributo 45, y que de hecho es un valor bastante atípico, teniendo tan solo 30 valores por debajo de el, y muy alejado del primer cuartil.

```{r}
print(paste0("NA restante: ",df_raw[is.na(df_raw$Attr60) & !is.na(df_raw$Attr45),"Attr45"]))
summary(df_raw$Attr45[!is.na(df_raw$Attr45)])
print(paste0("Nº valores menores al NA: ",sum(df_raw$Attr45[!is.na(df_raw$Attr45)]< -58)))
```

Sospecho que esa fila debe ser defectuosa, pues habiendo 592 NAs, y coincidiendo 591, no creo que el que falla sea un contraejemplo, sino un error posiblemente humano. Por este motivo, esa fila la eliminaremos e imputaremos el resto de filas. Veamos además, que estas dos distribuciones también son multiescala.

```{r}
my_summary <- function(x) {
  stats <- summary(x)
  stats <- stats[c("Min.", "1st Qu.", "Median", "3rd Qu.", "Max.")]
  round(stats,2)
}

```

```{r}
df_raw=df_raw[!(is.na(df_raw$Attr60) & !is.na(df_raw$Attr45)),]
print("Summary Atributo 45")
my_summary(df_raw$Attr45)
print("Número de elementos atributo 45 a distintas escalas")
print(paste0("Nº>10: ",sum(df_raw$Attr45>10 & !is.na(df_raw$Attr45))))
print(paste0("Nº>100: ",sum(df_raw$Attr45>100 & !is.na(df_raw$Attr45))))
print("Summary Atributo 60")
my_summary(df_raw$Attr60)
print("Número de elementos atributo 60 a distintas escalas")
print(paste0("Nº>10: ",sum(df_raw$Attr60>10 & !is.na(df_raw$Attr60))))
print(paste0("Nº>100: ",sum(df_raw$Attr60>100 & !is.na(df_raw$Attr60))))
print(paste0("Nº>1000: ",sum(df_raw$Attr60>1000 & !is.na(df_raw$Attr60))))
```

Para el atributo 45 vemos que la mayoría de los valores se encuentran entre 0 y 1, pero por encima de 10 hay 340 y por encima de 100 sigue habiendo 66(0.6%). Cabe destacar que también tiene valores negativos, pero por debajo de -10 solo tiene 125, y por debajo de -100 tiene 20.

Por otro lado el atributo 60 está principalmente en el rango 5-20 y hay 554 valores mayores a 100 y 94 valores mayores a 1000.

Por estos motivos, a ambos les aplicaremos una transformación logarítmica por el lado positivo, y para el atributo 45 acotaremos por -10 para los valores negativos. Por comodidad, al atributo 60 le aplicaremos log(1+x) a todo el dataset (para evitar problemas con el 0), mientras que al atributo 45 lo haremos solo en el lado positivo.

```{r}
df_raw[df_raw$Attr45< -10 & !is.na(df_raw$Attr45),"Attr45"]= -10
df_raw[df_raw$Attr45>0 & !is.na(df_raw$Attr45),"Attr45"]=log(1+df_raw[df_raw$Attr45>0 & !is.na(df_raw$Attr45),"Attr45"])

df_raw[!is.na(df_raw$Attr60),"Attr60"]=log(1+df_raw[!is.na(df_raw$Attr60),"Attr60"])
```

Ahora imputemos los valores faltantes. Los del atributo 60 como provienen de una distribución positiva, los imputaremos todos por el máximo (pues es una división por 0). Por otro lado, los NAs del atributo 45 los imputaremos con Knn pues es una distribución con valores negativos.

```{r}
imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = "Attr45")
# Reemplazar la columna original con la columna imputada
df_raw$Attr45 <- imputed_data$Attr45

df_raw[is.na(df_raw$Attr60),"Attr60"]=max(df_raw$Attr60,na.rm=T)
```

### Atributos 28,53,54,64

Los siguientes atributos tienen todos 228 NAs y el mismo denominador en común:

-   *Attr28 capital circulante / inmovilizado*

-   *Attr53 fondos propios / inmovilizado*

-   *Attr54 capital constante / inmovilizado*

-   *Attr64 ventas / inmovilizado*

Además, tenemos el Attr 24 que tiene 227 NAs (sospechosamente cerca del 228), aunque con un denominador distinto: Attr24 beneficio bruto (en 3 años) / activo total

```{r}
colSums(is.na(df_raw))[colSums(is.na(df_raw))>210]
```

A continuación vemos que los 228 NAs son compartidos por los 4 atributos, mientras que tal y como predijimos, que el atributo 24 tenga 227 es mera casualidad, pues en la matriz de confusión vemos que no coinciden con los 228 del otro.

```{r}
sum(is.na(df_raw$Attr28) & is.na(df_raw$Attr53) & is.na(df_raw$Attr54) & is.na(df_raw$Attr64))
table(is.na(df_raw$Attr24), is.na(df_raw$Attr28))
```

```{r}
summary(df_raw$Attr28)
print("Número de elementos atributo 28 a distintas escalas")
print(paste0("Nº>10: ",sum(df_raw$Attr28>10 & !is.na(df_raw$Attr28))))
print(paste0("Nº>100: ",sum(df_raw$Attr28>100 & !is.na(df_raw$Attr28))))
summary(df_raw$Attr53)
print("Número de elementos atributo 53 a distintas escalas")
print(paste0("Nº>10: ",sum(df_raw$Attr53>10 & !is.na(df_raw$Attr53))))
print(paste0("Nº>100: ",sum(df_raw$Attr53>100 & !is.na(df_raw$Attr53))))
summary(df_raw$Attr54)
print("Número de elementos atributo 54 a distintas escalas")
print(paste0("Nº>10: ",sum(df_raw$Attr54>10 & !is.na(df_raw$Attr54))))
print(paste0("Nº>100: ",sum(df_raw$Attr54>100 & !is.na(df_raw$Attr54))))
summary(df_raw$Attr64)
print("Número de elementos atributo 64 a distintas escalas")
print(paste0("Nº>10: ",sum(df_raw$Attr64>10 & !is.na(df_raw$Attr64))))
print(paste0("Nº>100: ",sum(df_raw$Attr64>100 & !is.na(df_raw$Attr64))))
print(paste0("Nº>1000: ",sum(df_raw$Attr64>1000 & !is.na(df_raw$Attr64))))
```

Como podemos ver, los atributos 28,53 y 54 están principalmente en el rango 0-2; sin embargo, tienen muchos valores en las escalas de 10 y 100. Por otro lado el atributo 64 se encuentra principalmente en el rango 0-10, y sigue teniendo bastantes valores por encima de 100 incluso llegando algunos a por encima de 100, sigue siendo multiescala pero menos pronunciada que las demás. Veamos además, que los primeros 3 atributos tienen en torno al 25% de sus valores inferiores a 0. Sin embargo, como veremos a continuación, no tienen tantos valores por debajo de -10.

```{r}
print("Nº valores inferiores a -10 de los atributos 28,53 y 54:")
sum(df_raw$Attr28< -10 & !is.na(df_raw$Attr28))
sum(df_raw$Attr53< -10 & !is.na(df_raw$Attr53))
sum(df_raw$Attr54< -10 & !is.na(df_raw$Attr54))
print("Nº valores inferiores a -100 de los atributos 28,53 y 54:")
sum(df_raw$Attr28< -100 & !is.na(df_raw$Attr28))
sum(df_raw$Attr53< -100 & !is.na(df_raw$Attr53))
sum(df_raw$Attr54< -100 & !is.na(df_raw$Attr54))
```

Para los atributos 28,53 y 54, como los valores negativos no se van tantas escalas hacia abajo, los limitaremos a -10; mientras que los valores positivos les aplicaremos log(1+x). Esto últio también se lo haremos al atributo 64.

```{r}
df_raw[df_raw$Attr28< -10 & !is.na(df_raw$Attr28),"Attr28"]= -10
df_raw[df_raw$Attr28>0 & !is.na(df_raw$Attr28),"Attr28"]=log(1+df_raw[df_raw$Attr28>0 & !is.na(df_raw$Attr28),"Attr28"])

df_raw[df_raw$Attr53< -10 & !is.na(df_raw$Attr53),"Attr53"]= -10
df_raw[df_raw$Attr53>0 & !is.na(df_raw$Attr53),"Attr53"]=log(1+df_raw[df_raw$Attr53>0 & !is.na(df_raw$Attr53),"Attr53"])

df_raw[df_raw$Attr54< -10 & !is.na(df_raw$Attr54),"Attr54"]= -10
df_raw[df_raw$Attr54>0 & !is.na(df_raw$Attr54),"Attr54"]=log(1+df_raw[df_raw$Attr54>0 & !is.na(df_raw$Attr54),"Attr54"])

df_raw[!is.na(df_raw$Attr64),"Attr64"]=log(1+df_raw[!is.na(df_raw$Attr64),"Attr64"])
```

Visto esto, imputaremos mediante Knn las variables 28, 53 y 54, y por el máximo para la variable 64.

```{r}
imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = c("Attr28","Attr53","Attr54"))
# Reemplazar la columna original con la columna imputada
df_raw$Attr28 <- imputed_data$Attr28
df_raw$Attr53 <- imputed_data$Attr53
df_raw$Attr54 <- imputed_data$Attr54

df_raw[is.na(df_raw$Attr64),"Attr64"]=max(df_raw$Attr64,na.rm=T)
```

### Atributo 24

Podemos ver que el atributo 24 y el atributo 25 tienen el mismo denominador, que son:

-   *Attr24 beneficio bruto (en 3 años) / activo total*

-   *Attr25 (fondos propios - capital social) / activo total*

Por tanto, esta es la primera vez que no es posible que el NA sea debido a una división por 0, pues el atributo 25 no tiene NAs. Viendo el significado de la variable 24, podríamos pensar que los NAs se deban a que si la empresa tiene menos de 3 años, entonces le hayan colocado un NA en "beneficio bruto (en 3 años)". Sin embargo, podemos ver que hay 20 empresas con Attr24=0; esto no desmiente la hipótesis, pero si es algo a destacar.

Debido a que sospecho que tiene causa, pero no tengo datos con significado suficientemente similares como para contrastar las ideas (no hay mas datos históricos), usaremos Knn para imputar los NAs del atributo 24.

```{r}
sum(is.na(df_raw$Attr24))
sum(is.na(df_raw$Attr25))
```

Mirando la distribución a continuación, podemos ver que la mayor parte de los valores están entre 0-0.3, y si nos vamos a la siguiente escala no quedan apenas valores; es decir, fuera de ]-3,3[ no hay apenas valores. Por este motivo, para esta variable simplemente acotaremos los valores a -3,3 e imputaremos después con kNN.

```{r}
my_summary(df_raw$Attr24)

print(paste0("Nº valores inferiores a -3: ",sum(df_raw$Attr24< -3,na.rm=T)))
print(paste0("Nº valores superiores a 3: ",sum(df_raw$Attr24> 3,na.rm=T)))
```
```{r}
df_raw[!is.na(df_raw$Attr24) & df_raw$Attr24>3,"Attr24"]=3
df_raw[!is.na(df_raw$Attr24) & df_raw$Attr24< -3,"Attr24"]=-3

imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = "Attr24")
# Reemplazar la columna original con la columna imputada
df_raw$Attr24 <- imputed_data$Attr24
```


### Atributo 41

El siguiente atributo que analizaremos es:
Attr41: pasivo total / ((resultado de explotación + amortizaciones) * (12/365))

Inicialmente puede parecer que no hay ninguna forma de comprobar si se debe a que el numerador sea 0. Sin embargo mirando los atributos:

Attr22 resultado de explotación / activo total
Attr48 EBITDA (beneficio de las actividades de explotación - amortizaciones) / activo total

Vemos que todos los NAs coinciden con 0s del atributo 22, por lo que basta averiguar si amortizaciones es 0 en esas filas. Resulta que amortizaciones no están solas en el atributo 48, pero coinciden que son 0 para todos los NAs del atributo 41. Por este motivo, consideraremos que estos NAs se deben a división por 0.

```{r}
table(df_raw$Attr22==0,is.na(df_raw$Attr41))
table(df_raw$Attr48==0,is.na(df_raw$Attr41))
```
Antes de imputar analizaremos los outliers para ver si aplicamos alguna transformación al atributo 41.

```{r}
my_summary(df_raw$Attr41)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr41[!is.na(df_raw$Attr41)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr41[!is.na(df_raw$Attr41)]>1)))
print(paste0("Nº valores superiores a 10: ",sum(df_raw$Attr41[!is.na(df_raw$Attr41)]>10)))
```
Como podemos ver, la mayor parte de la distribución está entre 0-0.2, y no hay demasiados fuera de 0-1. Para ahorrarme transformaciones y líos con números negativos, aprovecharemos que son valores en general en ]-1,1[ y le aplicaremos una tanh a este atributo, reduciéndolo así de manera estricta al intervalo -1,1. Además, los valores faltantes los imputaremos con kNN.

```{r}
df_raw[!is.na(df_raw$Attr41),"Attr41"]=tanh(df_raw[!is.na(df_raw$Attr41),"Attr41"])

imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = "Attr41")
# Reemplazar la columna original con la columna imputada
df_raw$Attr41 <- imputed_data$Attr41
```

### Atributo 32,47 y 52

A continuación analizaremos el atributo:
Attr32 (pasivo corriente * 365) / coste de los productos vendidos

Este atributo tiene el mismo denominador que los atributos:
Attr47 (existencias * 365) / coste de los productos vendidos
Attr52 (pasivo a corto plazo * 365) / coste de los productos vendidos)

```{r}
table(is.na(df_raw$Attr32),is.na(df_raw$Attr47))
table(is.na(df_raw$Attr32),is.na(df_raw$Attr52))
```
Como podemos ver estos tres atributos parecen tener 86 NAs en común, pero mirándolo mas detenidamente son solo 85, pues los atributos 47 y 52 difieren en un valor.
```{r}
table(is.na(df_raw$Attr47),is.na(df_raw$Attr52))
```
Mirando los 16 valores faltantes del 32 en los atributos 47 y 52, podemos ver que son no nulos, y en general altos respectos a su distribución. Concluimos que estos NAs no son necesariamente debido a una división por 0. De hecho, me parece raro que un producto pueda valer 0, aunque por supuesto podría ser un producto como un app que descargas gratis. Como no sabemos mas acerca de que puede haber causado esos NAs, los trataremos mediante imputación kNN.
```{r}
sum(df_raw$Attr52[!is.na(df_raw$Attr52) & is.na(df_raw$Attr32)]==0)
sum(df_raw$Attr47[!is.na(df_raw$Attr47) & is.na(df_raw$Attr32)]==0)
```

```{r}
print("Atributo 32:")
my_summary(df_raw$Attr32)
print(paste0("Nº valores inferiores a 0: ",sum(df_raw$Attr32<0,na.rm=T)))
print(paste0("Nº valores superiores a 1000: ",sum(df_raw$Attr32>1000,na.rm=T)))

print("Atributo 47:")
my_summary(df_raw$Attr47)
print(paste0("Nº valores inferiores a 0: ",sum(df_raw$Attr47<0,na.rm=T)))
print(paste0("Nº valores superiores a 250: ",sum(df_raw$Attr47>250,na.rm=T)))

print("Atributo 52:")
my_summary(df_raw$Attr52)
print(paste0("Nº valores inferiores a 0: ",sum(df_raw$Attr52<0,na.rm=T)))
print(paste0("Nº valores superiores a 10: ",sum(df_raw$Attr52>10,na.rm=T)))
```
Como podemos ver estas tres variables no tienen apenas valores negativos por lo que los imputaremos todos a 0 (así nos ahorramos algunos problemas como el -9000 en el atributo 32). Luego imputaremos por el valor grande que hemos mirando en cada una de las variables para ver si tenían una cantidad excesiva de outliers extremos (que vemos que no son demasiados).
```{r}
df_raw[!is.na(df_raw$Attr32) & df_raw$Attr32<0,"Attr32"]=0
df_raw[!is.na(df_raw$Attr32) & df_raw$Attr32>1000,"Attr32"]=1000
df_raw[!is.na(df_raw$Attr47) & df_raw$Attr47<0,"Attr47"]=0
df_raw[!is.na(df_raw$Attr47) & df_raw$Attr47>250,"Attr47"]=0
df_raw[!is.na(df_raw$Attr52) & df_raw$Attr52<0,"Attr52"]=0
df_raw[!is.na(df_raw$Attr52) & df_raw$Attr52>10,"Attr52"]=0
```
Como mencionamos anteriormente, los NAs de estas variables los imputaremos con kNN:
```{r}
imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = c("Attr32","Attr47","Attr52"))
# Reemplazar la columna original con la columna imputada
df_raw$Attr32 <- imputed_data$Attr32
df_raw$Attr47 <- imputed_data$Attr47
df_raw$Attr52 <- imputed_data$Attr52
```


### Atributos 13,19,20,23,30,31,39,42,43,44,49,56,62 y el 58

Los siguientes atributos que veremos son:

Attr13 (beneficio bruto + amortizaciones) / ventas
Attr19 beneficio bruto / ventas
Attr20 (existencias * 365) / ventas
Attr23 beneficio neto / ventas
Attr30 (pasivo total - tesorería) / ventas
Attr31 (beneficio bruto + intereses) / ventas
Attr39 beneficio sobre ventas / ventas
Attr42 resultado de explotación / ventas
Attr43 rotación de deudores + rotación de existencias en días
Attr44 (créditos * 365) / ventas
Attr49 EBITDA (resultado de las actividades de explotación - amortizaciones) / ventas
Attr56 (ventas - coste de los productos vendidos) / ventas
Attr62 (pasivo a corto plazo *365) / ventas

Miramos estos porque todos tienen 43 NAs y además, salvo el 43, todos tienen como denominador "ventas".
```{r}
colSums(is.na(df_raw))[colSums(is.na(df_raw))>30]
```
Podemos comprobar que estas 13 variables tienen los NAs en las mismas filas. Es evidente a que esto se debe a que ventas=0.
```{r}
sum(is.na(df_raw$Attr13)&is.na(df_raw$Attr19)&is.na(df_raw$Attr20)&is.na(df_raw$Attr23)&is.na(df_raw$Attr30)&is.na(df_raw$Attr31)&is.na(df_raw$Attr39)&is.na(df_raw$Attr42)&is.na(df_raw$Attr43)&is.na(df_raw$Attr44)&is.na(df_raw$Attr49)&is.na(df_raw$Attr56)&is.na(df_raw$Attr62))
```
El atributo 43 entiendo que se puede interpretar como el tiempo que se tarda en vender los productos, por lo que si ventas=0, este valor creo que debería ser 0. Por este motivo, imputaremos los del denominador por el máximo (tras el análisis de outliers) y el de 43 lo imputaremos directamente por 0.

```{r}
print("Atributo 13:")
my_summary(df_raw$Attr13)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr13[!is.na(df_raw$Attr13)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr13[!is.na(df_raw$Attr13)]> 1)))
print("Atributo 19:")
my_summary(df_raw$Attr19)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr19[!is.na(df_raw$Attr19)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr19[!is.na(df_raw$Attr19)]> 1)))
print("Atributo 20:")
my_summary(df_raw$Attr20)
print(paste0("Nº valores superiores a 300: ",sum(df_raw$Attr20[!is.na(df_raw$Attr20)]> 300)))
print("Atributo 23:")
my_summary(df_raw$Attr23)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr23[!is.na(df_raw$Attr23)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr23[!is.na(df_raw$Attr23)]> 1)))
print("Atributo 30:")
my_summary(df_raw$Attr30)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr30[!is.na(df_raw$Attr30)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr30[!is.na(df_raw$Attr30)]> 1)))
print(paste0("Nº valores superiores a 3: ",sum(df_raw$Attr30[!is.na(df_raw$Attr30)]> 3)))
print("Atributo 31:")
my_summary(df_raw$Attr31)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr31[!is.na(df_raw$Attr31)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr31[!is.na(df_raw$Attr31)]> 1)))
print("Atributo 39:")
my_summary(df_raw$Attr39)
print(paste0("Nº valores inferiores a -0.5: ",sum(df_raw$Attr39[!is.na(df_raw$Attr39)]< -0.5)))
print(paste0("Nº valores superiores a 0.5: ",sum(df_raw$Attr39[!is.na(df_raw$Attr39)]> 0.5)))
print("Atributo 42:")
my_summary(df_raw$Attr42)
print(paste0("Nº valores inferiores a -0.5: ",sum(df_raw$Attr42[!is.na(df_raw$Attr42)]< -0.5)))
print(paste0("Nº valores superiores a 0.5: ",sum(df_raw$Attr42[!is.na(df_raw$Attr42)]> 0.5)))
print("Atributo 43:")
my_summary(df_raw$Attr43)
print(paste0("Nº valores inferiores a 0: ",sum(df_raw$Attr43[!is.na(df_raw$Attr43)]< 0)))
print(paste0("Nº valores superiores a 500: ",sum(df_raw$Attr43[!is.na(df_raw$Attr43)]> 500)))
print("Atributo 44:")
my_summary(df_raw$Attr44)
print(paste0("Nº valores inferiores a 0: ",sum(df_raw$Attr44[!is.na(df_raw$Attr44)]< 0)))
print(paste0("Nº valores superiores a 250: ",sum(df_raw$Attr44[!is.na(df_raw$Attr44)]>250)))
print("Atributo 49:")
my_summary(df_raw$Attr49)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr49[!is.na(df_raw$Attr49)]< -1)))
print(paste0("Nº valores superiores a 0.5: ",sum(df_raw$Attr49[!is.na(df_raw$Attr49)]> 0.5)))
print("Atributo 56:")
my_summary(df_raw$Attr56)
print(paste0("Nº valores inferiores a -1: ",sum(df_raw$Attr56[!is.na(df_raw$Attr56)]< -1)))
print(paste0("Nº valores superiores a 1: ",sum(df_raw$Attr56[!is.na(df_raw$Attr56)]> 1)))
print("Atributo 62:")
my_summary(df_raw$Attr62)
print(paste0("Nº valores inferiores a 0: ",sum(df_raw$Attr62[!is.na(df_raw$Attr62)]< 0)))
print(paste0("Nº valores superiores a 10: ",sum(df_raw$Attr62[!is.na(df_raw$Attr62)]> 500)))

```
Como podemos ver, los atributos 13,19,23,30,31,39,42,49 y 56 están mayoritariamente comprendidos entre ]-1,1[ (el 30 llega mas hasta el 3). Por tanto, por comodidad, le aplicaremos a todos estos una tanh(x) para transformarlos estrictamente a ]-1,1[. Al resto; es decir, los atributos 20,43,44 y 62, los acotaremos inferiormente por 0 (apenas tienen valores inferiores a 0) y luego les aplicaremos log(1+x) pues tienen muchos valores a diversas escalas.

```{r}
df_raw[is.na(df_raw$Attr43),"Attr43"]=0
cols1=paste0("Attr",c(13,19,23,30,31,39,42,49,56))
df_raw[!is.na(df_raw$Attr13),cols1]=tanh(df_raw[!is.na(df_raw$Attr13),cols1])

cols2=paste0("Attr",c(20,43,44,62))
for(col in cols2)
{
  df_raw[!is.na(df_raw[,col]) & df_raw[,col]<0,col]=0
  df_raw[!is.na(df_raw[,col]),col]=log(1+df_raw[!is.na(df_raw[,col]),col])
}

```

Ahora haremos la imputacion, que como mencionamos será con el máximo para todas salvo la 43 que ya se hizo.

```{r}
cols=paste0("Attr",c(13,19,23,30,31,39,42,49,56,20,44,62))
for (col in cols)
{
  df_raw[is.na(df_raw[,col]),col]=max(df_raw[,col],na.rm=T)
}
```

### Atributos 4,8,12,16,17,26,33,34,40,46,50,63

-   Los siguientes valores faltantes a faltar son los de las variables:

-   *Attr4 activo circulante / pasivo a corto plazo*

-   *Attr8 valor contable de los fondos propios / pasivo total*

-   *Attr12 beneficio bruto / pasivo a corto plazo*

-   *Attr16 (beneficio bruto + amortización) / total pasivo*

-   *Attr17 total activo / total pasivo*

-   *Attr26 (beneficio neto + amortizaciones) / total pasivo*

-   *Attr33 gastos de explotación / pasivo a corto plazo*

-   *Attr34 gastos de explotación / total pasivo*

-   *Attr40 (activo circulante - existencias - deudores) / pasivo a corto plazo*

-   *Attr46 (activo circulante - existencias) / pasivo a corto plazo*

-   *Attr50 activo circulante / pasivo total Attr63 ventas / pasivo a corto plazo*

Como podemos ver, todas están divididas por pasivo a corto plazo o pasivo total. Además, todas tienen 14 o 18 NAs.

```{r}
cols=paste0("Attr",c(4,8,12,16,17,26,33,34,40,46,50,63))
colSums(is.na(df_raw[,cols]))[colSums(is.na(df_raw[,cols]))>1]
```

```{r}
cols18=paste0("Attr",c(4,12,33,40,46,63))

aux=rep(1,dim(df_raw)[1])
for(col in cols)
  aux=aux*is.na(df_raw[,col])
sum(aux)
aux=rep(1,dim(df_raw)[1])
for(col in cols18)
  aux=aux*is.na(df_raw[,col])
sum(aux)
```

Como podemos ver los todos tienen los 14 NAs en común, y además, los que tienen 18 NAs los tienen en común. Por tanto, podemos afirmar que esos NAs son debidos a división por 0. Por tanto, veamos ahora la distribución de cada una de estas variables para tratar los outliers en caso de ser necesario y luego imputar por el máximo cuando sea posible.

A continuación mostraremos un resumen de cada una de las variables indicando cuantos valores menor que -0.1 tienen (notado como N_men_0). Lo hacemos así para no hacerlo debajo de 0 estricto por si hay valores muy cercanos a cero pero ligeramente inferiores como -0.001.

También mostraremos el número de outliers por debajo del 1º cuartil y por encima del 3º cuartil notándolos como N_outliers "inf" y "sup" respectivamente.

```{r}
x=rbind()
for(col in cols)
{
  x=rbind(x,
          col=c(my_summary(df_raw[,col]),N_men_0=sum(df_raw[,col]< -0.1,na.rm=T),
                N_outliers_inf=num_outliers(df_raw[!is.na(df_raw[,col]),col])[1],
          N_outliers_sup=num_outliers(df_raw[!is.na(df_raw[,col]),col])[2]))
  rownames(x)[nrow(x)]=col
}
x
```

Como podemos ver, todas las variables tienen una cola por la derecha pesada, pues se ve que tienen una gran cantidad de "outliers" (valores superiores a Q3+1.5\*IQR). Además, los atributos 4,17,33,40,46,50 y 63 no tienen muy pocos valores negativos, por lo que los acotaremos por 0, y luego les aplicaremos log(1+x) para tratar las valores grandes. Sus NAs los imputaremos con los máximos que acaben habiendo al final.

Los atributos 8 y 34 presentan valores negativos pero no son apenas marcados como outliers, por lo que solo aplicaremos log(1+x) a los valores positivos, y a los outliers negativos los acotaremos por 1ºcuartil-1.5 IQR (pues vemos que son outliers muy marcados pues son del orden de 1000 frente al intervalo de los cuartiles de 0-0.2).

Finalmente, los atributos 12,16 y 26 les aplicaremos sgn(x)\*log(1+\|x\|) para poder tratar con las colas.

Para estos últimos 5 atributos (8,12,16,26 y 33) imputaremos mediante kNN los NAs por no saber si deben ser o no positivo.

```{r}
cols_pos=paste0("Attr",c(4,17,33,40,46,50,63))
for(col in cols_pos)
{
  df_raw[!is.na(df_raw[,col])  & df_raw[,col]<0,col]=0
  df_raw[!is.na(df_raw[,col]),col]=log(1+df_raw[!is.na(df_raw[,col]),col])
  df_raw[is.na(df_raw[,col]),col]=max(df_raw[,col],na.rm=T)
}

cols_2=paste0("Attr",c(8,33,34))
for(col in cols_2)
{
  q1=quantile(df_raw[!is.na(df_raw[,col]),col],0.25)
  q3=quantile(df_raw[!is.na(df_raw[,col]),col],0.75)
  iqr <- q3 - q1
  lower <- q1 - 1.5 * iqr
  df_raw[!is.na(df_raw[,col]) & df_raw[,col]<lower,col]=lower
  
  df_raw[!is.na(df_raw[,col]) & df_raw[,col]>0,col]=log(1+df_raw[!is.na(df_raw[,col]) & df_raw[,col]>0,col])
}

cols_3=paste0("Attr",c(12,16,26))
for(col in cols_3)
{
  df_raw[!is.na(df_raw[,col]),col]=
    sign(df_raw[!is.na(df_raw[,col]),col])*log(1+abs(df_raw[!is.na(df_raw[,col]),col]))
}

imputed_data <- VIM::kNN( subset(df_raw, select = -c(class)), k = 5, variable = c(cols_2,cols_3))

for(col in c(cols_2,cols_3))
{
  df_raw[,col]=imputed_data[,col]
}
```

### Atributo 58

El atributo que veremos a continuación es: Attr58 costes totales / ventas totales

que tiene 29 NAs, que podemos ver con la siguiente tabla que parcialmente son debido a división una división por cero gracias al atributo: Attr36 ventas totales / activo total

Curiosamente anteriormente hicimos imputacion de 43 filas que concluimos que se debía a ventas=0. Sin embargo, este se llama ventas totales, por lo que debe haber alguna diferencia. Miremos ahora la confusion matrix de que el atributo 36 sea 0 y el atributo 58 sea NA. Aquí vemos algo muy sorprendente, pues si el atributo 36 es 0, el atributo 58 debería ser NA (o 0 si se considera 0/0). Esto podemos fácilmente comprobar que no es el caso, lo que parece indicar que estas dos variables no están relacionadas, que hay algún error entre ellas o que la descripción de las variables está mal. Como no está claro la fuente de los NAs, los imputaremos con kNN.

```{r}
table(df_raw$Attr36==0,is.na(df_raw$Attr58))
```

Como vemos a continuación sigue una distribución prácticamente positiva y tiene una cola fuerte. Por este motivo acotaremos por 0 y le aplicaremos log(1+x).

```{r}
my_summary(df_raw$Attr58)
print(paste0("Nº de valores inferior a 0: ",sum(df_raw$Attr58<0,na.rm=T)))
print(paste0("Nº de outliers altos: ", num_outliers(df_raw$Attr58[!is.na(df_raw$Attr58)])[2]))
```

```{r}
df_raw[!is.na(df_raw[,"Attr58"])  & df_raw[,"Attr58"]<0,"Attr58"]=0
df_raw[!is.na(df_raw[,"Attr58"]),"Attr58"]=log(1+df_raw[!is.na(df_raw[,"Attr58"]),"Attr58"])
```

### Atributos 5,9,15,58 y 61

Las variables con NAs restantes son:

-   *Attr5 [(tesorería + valores a corto plazo + títulos de crédito - pasivo a corto plazo) / (gastos de explotación - amortizaciones)] \* 365 \* 365*

-   *Attr9 ventas / activo total*

-   *Attr15 (pasivo total \* 365) / (beneficio bruto + amortizaciones)*

-   *Attr58 costes totales / ventas totales*

-   *Attr61 ventas / deudores*

Mostramos a continuación cuantos NAs tiene cada una y una gráfica que muestra si comparte alguna fila:

```{r}
colSums(is.na(df_raw))[colSums(is.na(df_raw))>0]
aggr(df_raw[rowSums(is.na(df_raw))>0,])
```

Vemos que en general las filas que tienen NAs en general tienen 1 o 2 valores con NAs. De hecho, solo parece haber una fila con mas de 2 NAs, esas las eliminaremos y el resto las imputaremos con kNN.

```{r}
df_raw <- df_raw[!(is.na(df_raw$Attr5)&is.na(df_raw$Attr9)&is.na(df_raw$Attr15)),]

cols=paste0("Attr",c(5,9,15,58,61))
imputed_data <- VIM::kNN(subset(df_raw, select = -c(class)), k = 5, variable =cols)

for(col in cols)
{
  df_raw[,col]=imputed_data[,col]
}

if(sum(!complete.cases(df_raw))!=0)
  print("Ha habido algún error/ todavía quedan NAs")
```

## Tratamiento outliers restantes

Hemos terminado de imputar los valores faltantes y ahora solo queda analizar varias variables para mirar sus outliers y si vamos a aplicarles alguna transformación para modificar su distribución (quitar colas pesadas). Las variables que nos quedan por mirar son los atributos:

1,2,3,5,6,7,9,10,11,14,15,18,22,25,29,35,36,38,48,51,55,57,59 y 61.

```{r}
cols=paste0("Attr",c(1,2,3,5,6,7,9,10,11,14,15,18,22,25,29,35,36,38,48,51,55,57,59,61))
x=rbind()
for(col in cols)
{
  aux=df_raw[,col]
  x=rbind(x,
          col=c(my_summary(aux),N_men_0=sum(aux< -0.1),
                N_outliers_inf=num_outliers(aux)[1],
          N_outliers_sup=num_outliers(aux)[2]))
  rownames(x)[nrow(x)]=col
}
x
```

Como podemos los atributos 1,5,6,7,11,14,15,18,22,35,48,55, 57 y 59 tienen muchos outliers tanto inferiores como superiores, donde los inferiores son menor que 0. Por este motivo, a estas variables les aplicaremos sign(x)\*log(1+\|x\|), pues estos outliers realmente son un síntoma de que las colas de sus distribuciones son muy pesadas al ser en la mayoría de los casos en torno al 10% de las filas.

Por otro lado, los atributos 2, 9, 29, 36, 51 y 61 tiene solo colas pesadas positivas y además no tienen muy pocos valores negativas (2 como mucho). Por tanto, estas variables las acotaremos inferiormente por 0 y luego les aplicaremos log(1+x).

Finalmente, los atributos 3, 10, 25 y 38 tienen colas negativas muy pesadas, y algún outlier positivo. Por esta razón, acotaremos estas variables superiormente por 3ºcuantil+1.5\*IQR y a sus valores negativos les aplicaremos -log(1+\|x\|).

```{r}
cols_doble=paste0("Attr",c(1,5,6,7,11,14,15,18,22,35,48,55,57,59))
for(col in cols_doble)
{
  df_raw[,col]=sign(df_raw[,col])*log(1+abs(df_raw[,col]))
}

cols_pos=paste0("Attr",c(2,9,29,36,51,61))
for(col in cols_pos)
{
  df_raw[df_raw[,col]<0,col]=0
  df_raw[,col]=log(1+df_raw[,col])
}

cols_neg=paste0("Attr",c(3,10,25,38))
for(col in cols_neg)
{
  q1=quantile(df_raw[,col],0.25)
  q3=quantile(df_raw[,col],0.75)
  IQR=q3-q1
  upper=q3+1.5*IQR
  df_raw[df_raw[,col]>upper,col]=upper
  df_raw[df_raw[,col]<0,col]=-log(1+abs(df_raw[df_raw[,col]<0,col]))
  
}
```


```{r}
saveRDS(df_raw,"1_Procesado_outliers_NAs.Rds")
```



## Correlación entre variables

A lo largo de esta sección analizaremos la correlación que hay entre las variables y eliminaremos las variables mas correladas entre sí.

```{r}
df_raw[,"class"]=as.numeric(df_raw[,"class"])
library(correlation)
library(corrplot)

indexes=unique(which(corr_matrix >= 0.7 & row(corr_matrix) != col(corr_matrix), arr.ind = TRUE)[,c("row")])
corrplot(cor_sort(corr_matrix[indexes,indexes]))
```

Hemos representado la matriz de correlación de las distintas variables seleccionando solo aquellas que tienen al una correlación de al menos un 70% con otra variable. Además, las hemos ordenado para poder ver quienes están correladas entre sí.

Así inmediatamente vemos que los hay están los siguientes grupos de variables altamente correladas:

-   28,53 y 54

-   2,3,10,25,38 y 51

-   32,33,52 y 63.

-   4,8,17,40,46 y 50.

-   9 y 36.

-   13,19,23,31,39,42,49,56,

-   12,16 y 26.

-   1,7,11,14,18,22, 24,35 y 48
